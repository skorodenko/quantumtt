{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from ollama import chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_LEN = 500\n",
    "DATASET_FILE = Path(\"./test.ndjson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1_621, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Mountain</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;Mount Griggs&quot;</td></tr><tr><td>&quot;Diran&quot;</td></tr><tr><td>&quot;Kubi Gangri&quot;</td></tr><tr><td>&quot;Mount Massive&quot;</td></tr><tr><td>&quot;Dreiländerspitze&quot;</td></tr><tr><td>&hellip;</td></tr><tr><td>&quot;Liankang Kangri&quot;</td></tr><tr><td>&quot;Mount Moulton&quot;</td></tr><tr><td>&quot;Mount Lindsey&quot;</td></tr><tr><td>&quot;Ishpatina Ridge&quot;</td></tr><tr><td>&quot;Doi Ian&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1_621, 1)\n",
       "┌──────────────────┐\n",
       "│ Mountain         │\n",
       "│ ---              │\n",
       "│ str              │\n",
       "╞══════════════════╡\n",
       "│ Mount Griggs     │\n",
       "│ Diran            │\n",
       "│ Kubi Gangri      │\n",
       "│ Mount Massive    │\n",
       "│ Dreiländerspitze │\n",
       "│ …                │\n",
       "│ Liankang Kangri  │\n",
       "│ Mount Moulton    │\n",
       "│ Mount Lindsey    │\n",
       "│ Ishpatina Ridge  │\n",
       "│ Doi Ian          │\n",
       "└──────────────────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset with unique mountain names\n",
    "# Reference: https://www.kaggle.com/datasets/codefantasy/list-of-mountains-in-the-world\n",
    "\n",
    "mn_df = pl.read_csv(Path(\"Mountain.csv\"), columns=[\"Mountain\"])\n",
    "mn_df = mn_df.unique()\n",
    "mn_lst = mn_df[\"Mountain\"].to_list()\n",
    "mn_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset generation process\n",
    "\n",
    "I will use ollama to host llm locally (llama3.1)\n",
    "\n",
    "To make dataset diverse and reasonably 'realistic' I will use the following approach:\n",
    "\n",
    "1. 40% of the data will be stories/articles/news/tweets about mountain __explitcitly__ saying its mountain with mentioning its name\n",
    "2. 5% of the data will be stories/articles/news/tweets about mountain __explitcitly__ saying its mountain with mentioning its name (2-4 different mountain names)\n",
    "3. 15% of the data will be stories/articles/news/tweets with mentioning name of the mountain __omitting__ the fact that it is a mountain, so that names like 'Twin peaks' would be dependant on its context (whether its a TV show or a mountain name)\n",
    "4. 40% of the data will be random stories/articles/news/tweets\n",
    "\n",
    "Each text will be short: 1-2 sentences.\n",
    "\n",
    "P.S. Local generation of dataset with 1500 records took approx. 15 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [05:54<00:00,  1.41it/s]\n"
     ]
    }
   ],
   "source": [
    "from random import choices, randint, seed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set seed to maintain reproducible randomization\n",
    "seed(42)\n",
    "\n",
    "QUERY_TYPE = [1, 2, 3, 4]\n",
    "QUERY_WEIGHT = [0.4, 0.05, 0.15, 0.4]\n",
    "QUERY_THEME = [\"story\", \"article\", \"piece of news\", \"tweet\"]\n",
    "\n",
    "SEED_PROMPT = \"You are writer, for every prompt respond with 1-2 sentences.\"\n",
    "\n",
    "raw_dataset = []\n",
    "\n",
    "for i in tqdm(range(DATASET_LEN)):\n",
    "    qtype = choices(QUERY_TYPE, QUERY_WEIGHT, k=1)[0]\n",
    "    qtheme = choices(QUERY_THEME, k=1)[0]\n",
    "\n",
    "    if qtype == 1:\n",
    "        mname = choices(mn_lst)\n",
    "        query = f\"Write a {qtheme} about mountain {mname[0]}\"\n",
    "    elif qtype == 2:\n",
    "        mname = choices(mn_lst, k=randint(2,4))\n",
    "        query = f\"Write a {qtheme} about mountains {','.join(mname)}\"\n",
    "    elif qtype == 3:\n",
    "        mname = choices(mn_lst)\n",
    "        query = f\"Write a {qtheme} about {mname[0]}\"\n",
    "    elif qtype == 4:\n",
    "        mname = []\n",
    "        query = f\"Write a {qtheme} on a random topic\"\n",
    "    \n",
    "    response = chat(model=\"llama3.1\", messages=[\n",
    "        {\"role\": \"system\", \"content\": SEED_PROMPT}, \n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ], options={\"seed\": i})\n",
    "    \n",
    "    raw_dataset.append((qtype, response.message.content, mname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform raw dataset\n",
    "\n",
    "1. Tokenize into generic tokens (text -> dicrete words)\n",
    "2. Label tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "\n",
    "etypes = [\"O\", \"B-MNT\", \"I-MNT\"]\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "def raw_transform(record):\n",
    "    qtype, text, mnts = record\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    labels = [0] * len(tokens)\n",
    "    for mnt in mnts:\n",
    "        mtokens = tokenizer.tokenize(mnt.lower())\n",
    "        slabels = [1 if i == 0 else 2 for i in range(len(mtokens))]\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i:i+len(mtokens)] == mtokens:\n",
    "                labels[i:i+len(mtokens)] = slabels\n",
    "    return (qtype, tokens, labels)\n",
    "\n",
    "transformed_dataset = list(map(raw_transform, raw_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataset into the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(DATASET_FILE, \"x\") as fp:\n",
    "    for qtype, tokens, labels in transformed_dataset:\n",
    "        record = {\n",
    "            \"qtype\": qtype,\n",
    "            \"tokens\": tokens,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "        fp.write(json.dumps(record))\n",
    "        fp.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
